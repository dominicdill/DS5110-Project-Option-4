# DS5110-Project-Option-4

## Team members:
* Dominic Dill
* Casey Tilton
* Alexey Rizvanov

## 4 -- Institute for Experiential AI

* Stakeholder -- Mark Wagy, PhD, Senior Data Scientist at The Roux Instute
* Story/Goal: We build machine learning models on a lot of datasets for various partners and industries. We would like t o be able to have an automated process that we can run our algorithms against ground truth datasets to check whether the y are biased or not. In this project we propose building such a tool.
* Data: [Law School Admission Dataset](http://www.seaphe.org/databases.php)

Other EIA project ideas that need to be fleshed out...

* Notebook to report workflow: Build a Github Action workflow that generates a report from a series of Jupyter noteboo ks when annotated properly. Likely will make use of Scrapbook and Papermill; but open to other tooling.
* EAI Data Lake. Set up a Data Lake that is searchable and self-documenting. It must allow for unstructured, semistruc tured and structured data and be minimal cost that we can use to track public (and internal) data for use with projects.
* ML Deployment Pipeline. A pipeline for monitoring and deployment of machine learning models.
* Knowledge Graph for AI Solutions. Build a knowledge graph that can track, visualize and export entities and their re lationships with expandable features. This one probably needs a lot more explanation but it is a cool project.
* ML Bias Reference. Build a tool that generates synthetic dataset that we can use as an "ML Bias" point of reference. Operationally it would take in an ML algorithm and provides an indication of the level of bias in an ML algorithm with respect to a synthetic a reference dataset.
